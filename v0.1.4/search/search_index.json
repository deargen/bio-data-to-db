{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":[" "]},"docs":[{"location":"","title":"bio-data-to-db: make Uniprot PostgreSQL database","text":"<p>Bio data is a little messy to work with, and everybody deserves a clean database. This package helps you to convert bio data to a database.</p> <p>Partially written in Rust, thus equipped with an extremely fast uniprot xml parser. Packaged for python, so anyone can easily install and use it.</p> <p>This package focuses more on parsing the data and inserting it into the database, rather than curating the data. Main features include:</p> <ul> <li>Uniprot: Parse the uniprot xml file and insert the data into the database.</li> <li>BindingDB: Fix the HTML entities in the assay table.</li> <li>PostgreSQL Helpers: Useful functions to work with PostgreSQL.</li> <li>SMILES: Canonicalize SMILES strings.</li> <li>Polars: Useful functions to work with Polars.</li> </ul> <p>\ud83d\udcda Documentation has the API reference and the usage.</p>"},{"location":"#installation","title":"\ud83d\udee0\ufe0f Installation","text":"<pre><code>pip install bio-data-to-db\n</code></pre>"},{"location":"#usage","title":"\ud83d\udea6 Usage","text":"<p>You can use the command line interface or the python API.</p>"},{"location":"#uniprot","title":"Uniprot","text":"<pre><code># It will create a db 'uniprot' and a table named 'public.uniprot_info' in the database.\n# If you want another name, you can optionally pass it as the last argument.\nbio-data-to-db uniprot create-empty-table 'postgresql://username@localhost:5432/uniprot'\n\n# It will parse the xml file and insert the data into the table.\n# It requires that the table is already created.\nbio-data-to-db uniprot xml-to-postgresql '~/Downloads/uniprot_sprot.xml' 'postgresql://username@localhost:5432/uniprot'\n\n# Create a table that maps accession to pk_id.\n# Columns: accession (text), uniprot_pk_ids (integer[])\nbio-data-to-db uniprot create-accession-to-pk-id 'postgresql://username@localhost:5432/uniprot'\n\n# It will parse the keywords tsv file and insert the data into the table.\nbio-data-to-db uniprot keywords-tsv-to-postgresql '~/Downloads/keywords_all_2024_06_26.tsv' 'postgresql://username@localhost/uniprot'\n</code></pre> <pre><code>from bio_data_to_db.uniprot import (\n    create_accession_to_pk_id_table,\n    create_empty_table,\n    uniprot_xml_to_postgresql,\n    keywords_tsv_to_postgresql,\n)\n\ncreate_empty_table(\"postgresql://user:password@localhost:5432/uniprot\")\n# It requires that the table is already created.\nuniprot_xml_to_postgresql(\"~/Downloads/uniprot_sprot.xml\", \"postgresql://user:password@localhost:5432/uniprot\")\ncreate_accession_to_pk_id_table(\"postgresql://user:password@localhost:5432/uniprot\")\nkeywords_tsv_to_postgresql(\"~/Downloads/keywords_all_2024_06_26.tsv\", \"postgresql://user:password@localhost:5432/uniprot\")\n</code></pre>"},{"location":"#bindingdb","title":"BindingDB","text":"<pre><code># Decode HTML entities and strip the strings in the `assay` table (column: description and assay_name).\n# Currently, only assay table is supported.\nbio-data-to-db bindingdb fix-table assay 'mysql://username:password@localhost/bind'\n</code></pre> <pre><code>from bio_data_to_db.bindingdb.fix_tables import fix_assay_table\n\nfix_assay_table(\"mysql://username:password@localhost/bind\")\n</code></pre>"},{"location":"#postgresql-helpers-smiles-polars-utils-and-more","title":"PostgreSQL Helpers, SMILES, Polars utils and more","text":"<p>This package also provides some useful functions to work with PostgreSQL, SMILES, Polars and more.</p> <pre><code>from bio_data_to_db.utils.postgresql import (\n    create_db_if_not_exists,\n    create_schema_if_not_exists,\n    make_int_column_primary_key_identity,\n    make_columns_primary_key,\n    make_columns_unique,\n    make_large_columns_unique,\n    split_column_str_to_list,\n    polars_write_database,  # addressed issues with list columns\n)\n\nfrom bio_data_to_db.utils.smiles import (\n    canonical_smiles_wo_salt,\n    polars_canonical_smiles_wo_salt,\n)\n\nfrom bio_data_to_db.utils.fasta import (\n    polars_standardize_fasta,\n)\n\nfrom bio_data_to_db.utils.polars import (\n    w_pbar,\n)\n</code></pre> <p>You can find the usage in the \ud83d\udcda documentation.</p>"},{"location":"#maintenance-notes","title":"\ud83d\udc68\u200d\ud83d\udcbb\ufe0f Maintenance Notes","text":""},{"location":"#install-from-source","title":"Install from source","text":"<p>Install <code>uv</code>, <code>rustup</code> and <code>maturin</code>. Activate a virtual environment. Then,</p> <pre><code>bash scripts/install.sh\nuv pip install -r deps/requirements_dev.in\n</code></pre>"},{"location":"#generate-lockfiles","title":"Generate lockfiles","text":"<p>Use GitHub Actions: <code>apply-pip-compile.yml</code>. Manually launch the workflow and it will make a commit with the updated lockfiles.</p>"},{"location":"#publish-a-new-version-to-pypi","title":"Publish a new version to PyPI","text":"<p>Use GitHub Actions: <code>deploy.yml</code>. Manually launch the workflow and it will compile on all architectures and publish the new version to PyPI.</p>"},{"location":"#about-sqlx","title":"About sqlx","text":"<p>Sqlx offline mode should be configured so you can compile the code without a database present.</p> <p>First, you need to make sure the database is connected to enable the offline mode.</p> <pre><code># .env file\nDATABASE_URL=postgresql://postgres:password@localhost:5432/uniprot\nSQLX_OFFLINE=true  # force offline mode. Otherwise, use the DATABASE_URL to connect to the database.\n</code></pre> <p>The database needs to have the table. It can be empty but the structure has to be accessible. If it doesn't already exist, you can create the table with the following command.</p> <pre><code>bio-data-to-db uniprot create-empty-table 'postgresql://username@localhost:5432/uniprot'\n</code></pre> <p>Then, you can run the following command to prepare the SQL queries. This defines the type information for the queries.</p> <pre><code>cargo install sqlx-cli --no-default-features --features postgres\ncargo sqlx prepare\n</code></pre> <p>This will make <code>.sqlx/</code> directory with the type information for the queries.</p>"},{"location":"CHANGELOG/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"CHANGELOG/#v014-2024-08-29","title":"v0.1.4 - 2024-08-29","text":""},{"location":"CHANGELOG/#build-system","title":"Build System","text":"<ul> <li><code>c6b44ed</code> - use psycopg2-binary (commit by @kiyoon)</li> <li><code>d55bd99</code> - update requirements using uv pip compile [skip ci] (commit by @github-actions[bot])</li> </ul>"},{"location":"CHANGELOG/#v013-2024-08-20","title":"v0.1.3 - 2024-08-20","text":""},{"location":"CHANGELOG/#new-features","title":"New Features","text":"<ul> <li><code>2459171</code> - standardize fasta, multiple column primary key, fix (commit by @kiyoon)</li> </ul>"},{"location":"CHANGELOG/#v012-2024-08-01","title":"v0.1.2 - 2024-08-01","text":""},{"location":"CHANGELOG/#new-features_1","title":"New Features","text":"<ul> <li><code>6d9f4ad</code> - fix Binding DB assay html encoding, polars_canonical_smiles_wo_salt, ci and mkdocs (PR #2 by @kiyoon)</li> </ul>"},{"location":"CHANGELOG/#v011-2024-07-30","title":"v0.1.1 - 2024-07-30","text":""},{"location":"CHANGELOG/#bug-fixes","title":"Bug Fixes","text":"<ul> <li><code>a8daeb4</code> - safe SQL, perf of <code>polars_write_database</code> improved (PR #1 by @kiyoon)</li> </ul>"},{"location":"CHANGELOG/#v010-2024-07-17","title":"v0.1.0 - 2024-07-17","text":""},{"location":"CHANGELOG/#documentation-changes","title":"Documentation Changes","text":"<ul> <li><code>ab1143c</code> - clean up (commit by @kiyoon)</li> </ul>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li> bio_data_to_db<ul> <li> bindingdb<ul> <li> fix_tables</li> </ul> </li> <li> uniprot<ul> <li> utils</li> </ul> </li> <li> utils<ul> <li> fasta</li> <li> log</li> <li> polars</li> <li> postgresql</li> <li> smiles</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/bio_data_to_db/","title":"Index","text":""},{"location":"reference/bio_data_to_db/#bio_data_to_db.uniprot_xml_to_postgresql","title":"<code>uniprot_xml_to_postgresql(*, uniprot_xml_path, uri)</code>","text":"<p>(\ud83e\udd80 Rust) Load UniProt XML file into PostgreSQL database.</p> <p>This creates a <code>uniprot</code> database and a <code>uniprot_info</code> table.</p> Source code in <code>src/bio_data_to_db/bio_data_to_db.pyi</code> <pre><code>def uniprot_xml_to_postgresql(\n    *,\n    uniprot_xml_path: str,\n    uri: str,\n) -&gt; None:\n    \"\"\"\n    (\ud83e\udd80 Rust) Load UniProt XML file into PostgreSQL database.\n\n    This creates a `uniprot` database and a `uniprot_info` table.\n    \"\"\"\n</code></pre>"},{"location":"reference/bio_data_to_db/bindingdb/","title":"Index","text":""},{"location":"reference/bio_data_to_db/bindingdb/fix_tables/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> fix_tables","text":""},{"location":"reference/bio_data_to_db/bindingdb/fix_tables/#bio_data_to_db.bindingdb.fix_tables.fix_assay_table","title":"<code>fix_assay_table(uri)</code>","text":"<p>Fix the assay table in MySQL by decoding HTML entities like <code>&amp;#39;</code> and strip empty spaces.</p> Notes <ul> <li>the table is replaced.</li> <li>types are reserved by manually changing them to the original types. For example, varchar -&gt; text -&gt; varchar</li> <li>primary key and foreign key constraints are reserved by manually adding them back just like the original table</li> </ul> Source code in <code>src/bio_data_to_db/bindingdb/fix_tables.py</code> <pre><code>def fix_assay_table(uri: str):\n    \"\"\"\n    Fix the assay table in MySQL by decoding HTML entities like `&amp;#39;` and strip empty spaces.\n\n    Notes:\n        - the table is replaced.\n        - types are reserved by manually changing them to the original types. For example, varchar -&gt; text -&gt; varchar\n        - primary key and foreign key constraints are reserved by manually adding them back just like the original table\n    \"\"\"\n    query = \"\"\"\n        SELECT\n          *\n        FROM\n          assay\n    \"\"\"\n    assay_df = pl.read_database_uri(query=query, uri=uri)\n\n    # the column might be \"binary\" type if the type is \"TEXT\" in MySQL\n    assay_df = assay_df.with_columns(\n        pl.col(\"description\")\n        .cast(pl.Utf8)\n        .map_elements(lambda s: html.unescape(s.strip()), return_dtype=pl.Utf8),\n        pl.col(\"assay_name\")\n        .cast(pl.Utf8)\n        .map_elements(lambda s: html.unescape(s.strip()), return_dtype=pl.Utf8),\n    )\n\n    assay_df.write_database(\n        table_name=\"assay\",\n        connection=uri,\n        if_table_exists=\"replace\",\n    )\n\n    with sqlalchemy.create_engine(uri).connect() as conn:\n        conn.execute(\n            sqlalchemy.text(\"\"\"\n                ALTER TABLE assay MODIFY COLUMN `entryid` INT(11);\n                ALTER TABLE assay MODIFY COLUMN `assayid` INT(11);\n                ALTER TABLE assay MODIFY COLUMN `description` VARCHAR(4000);\n                ALTER TABLE assay MODIFY COLUMN `assay_name` VARCHAR(200);\n                ALTER TABLE assay ADD PRIMARY KEY (`entryid`,`assayid`);\n                ALTER TABLE assay ADD CONSTRAINT `assay_ibfk_1` FOREIGN KEY (`entryid`) REFERENCES `entry` (`entryid`);\n            \"\"\")\n        )\n</code></pre>"},{"location":"reference/bio_data_to_db/uniprot/","title":"Index","text":""},{"location":"reference/bio_data_to_db/uniprot/#bio_data_to_db.uniprot.uniprot_xml_to_postgresql","title":"<code>uniprot_xml_to_postgresql(*, uniprot_xml_path, uri)</code>","text":"<p>(\ud83e\udd80 Rust) Load UniProt XML file into PostgreSQL database.</p> <p>This creates a <code>uniprot</code> database and a <code>uniprot_info</code> table.</p> Source code in <code>src/bio_data_to_db/bio_data_to_db.pyi</code> <pre><code>def uniprot_xml_to_postgresql(\n    *,\n    uniprot_xml_path: str,\n    uri: str,\n) -&gt; None:\n    \"\"\"\n    (\ud83e\udd80 Rust) Load UniProt XML file into PostgreSQL database.\n\n    This creates a `uniprot` database and a `uniprot_info` table.\n    \"\"\"\n</code></pre>"},{"location":"reference/bio_data_to_db/uniprot/#bio_data_to_db.uniprot.create_accession_to_pk_id","title":"<code>create_accession_to_pk_id(uri)</code>","text":"<p>Create a table to map accession to uniprot_pk_id, from the uniprot_info table.</p> <p>It creates the following tables:</p> <ul> <li>accession_to_pk_id</li> <li>accession_to_pk_id_list</li> </ul> Note <p>The mapping is not unique. It is possible to have multiple uniprot_pk_id for a single accession and vice versa.</p> Source code in <code>src/bio_data_to_db/uniprot/utils.py</code> <pre><code>def create_accession_to_pk_id(uri: str):\n    \"\"\"\n    Create a table to map accession to uniprot_pk_id, from the uniprot_info table.\n\n    It creates the following tables:\n\n    - accession_to_pk_id\n    - accession_to_pk_id_list\n\n    Note:\n        The mapping is not unique. It is possible to have multiple uniprot_pk_id for a single accession and vice versa.\n    \"\"\"\n    with psycopg.connect(\n        conninfo=uri,\n    ) as conn:\n        try:\n            cursor = conn.cursor()\n            conn.autocommit = True\n            cursor.execute(\n                query=\"\"\"\n                CREATE TABLE public.accession_to_pk_id (\n                  accession TEXT,\n                  uniprot_pk_id BIGINT\n                )\n            \"\"\"\n            )\n            logger.info(\n                \"Table structure 'uniprot.public.accession_to_pk_id' created successfully\"\n            )\n\n            cursor.execute(\n                query=\"\"\"\n                INSERT INTO public.accession_to_pk_id (accession, uniprot_pk_id)\n                SELECT UNNEST(accessions), uniprot_pk_id\n                FROM public.uniprot_info\n            \"\"\"\n            )\n            logger.info(\n                \"Table 'uniprot.public.accession_to_pk_id' insert content successfully\"\n            )\n\n            cursor.execute(\n                query=\"\"\"\n                CREATE TABLE public.accession_to_pk_id_list (\n                  accession TEXT PRIMARY KEY,\n                  uniprot_pk_ids BIGINT[]\n                )\n            \"\"\"\n            )\n            logger.info(\n                \"Table structure 'uniprot.public.accession_to_pk_id_list' created successfully\"\n            )\n\n            cursor.execute(\n                query=\"\"\"\n                INSERT INTO public.accession_to_pk_id_list (accession, uniprot_pk_ids)\n                SELECT accession, ARRAY_AGG(uniprot_pk_id) AS uniprot_pk_ids\n                FROM public.accession_to_pk_id\n                GROUP BY accession;\n            \"\"\"\n            )\n            logger.info(\n                \"Table 'uniprot.public.accession_to_pk_id_list' content added successfully\"\n            )\n\n        except psycopg.Error:\n            logger.exception(\"Error creating table 'uniprot.public.accession_to_pk_id'\")\n</code></pre>"},{"location":"reference/bio_data_to_db/uniprot/#bio_data_to_db.uniprot.create_empty_table","title":"<code>create_empty_table(uri)</code>","text":"<p>Create an empty table in the database. Necessary to create the table structure before inserting data.</p> Note <p>It runs the following SQL query: <pre><code>CREATE TABLE public.uniprot_info (\n  uniprot_pk_id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,\n  accessions TEXT[],\n  names TEXT[],\n  protein_names TEXT[],\n  gene_names TEXT[],\n  organism_scientific TEXT,\n  organism_commons TEXT[],\n  organism_synonyms TEXT[],\n  ncbi_taxonomy_id INT,\n  deargen_ncbi_taxonomy_id INT,\n  lineage TEXT[],\n  keywords TEXT[],\n  geneontology_ids TEXT[],\n  geneontology_names TEXT[],\n  sequence TEXT,\n  deargen_molecular_functions TEXT[]\n)\n</code></pre></p> Source code in <code>src/bio_data_to_db/uniprot/utils.py</code> <pre><code>def create_empty_table(\n    uri: str,\n):\n    \"\"\"\n    Create an empty table in the database. Necessary to create the table structure before inserting data.\n\n    Note:\n        It runs the following SQL query:\n        ```sql\n        CREATE TABLE public.uniprot_info (\n          uniprot_pk_id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,\n          accessions TEXT[],\n          names TEXT[],\n          protein_names TEXT[],\n          gene_names TEXT[],\n          organism_scientific TEXT,\n          organism_commons TEXT[],\n          organism_synonyms TEXT[],\n          ncbi_taxonomy_id INT,\n          deargen_ncbi_taxonomy_id INT,\n          lineage TEXT[],\n          keywords TEXT[],\n          geneontology_ids TEXT[],\n          geneontology_names TEXT[],\n          sequence TEXT,\n          deargen_molecular_functions TEXT[]\n        )\n        ```\n    \"\"\"\n    uri_wo_dbname, dbname = uri.rsplit(\"/\", 1)\n    create_db_if_not_exists(uri_wo_dbname, dbname)\n    create_schema_if_not_exists(uri, \"public\")\n\n    with psycopg.connect(\n        conninfo=uri,\n    ) as conn:\n        try:\n            cursor = conn.cursor()\n            conn.autocommit = True\n            cursor.execute(\n                query=\"\"\"\n                CREATE TABLE public.uniprot_info (\n                  uniprot_pk_id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,\n                  accessions TEXT[],\n                  names TEXT[],\n                  protein_names TEXT[],\n                  gene_names TEXT[],\n                  organism_scientific TEXT,\n                  organism_commons TEXT[],\n                  organism_synonyms TEXT[],\n                  ncbi_taxonomy_id INT,\n                  deargen_ncbi_taxonomy_id INT,\n                  lineage TEXT[],\n                  keywords TEXT[],\n                  geneontology_ids TEXT[],\n                  geneontology_names TEXT[],\n                  sequence TEXT,\n                  deargen_molecular_functions TEXT[]\n                )\n            \"\"\"\n            )\n            logger.info(f\"Database '{dbname}.public.uniprot_info' created successfully\")\n\n        except psycopg.Error:\n            logger.exception(f\"Error creating database '{dbname}.public.uniprot_info'\")\n</code></pre>"},{"location":"reference/bio_data_to_db/uniprot/#bio_data_to_db.uniprot.keywords_tsv_to_postgresql","title":"<code>keywords_tsv_to_postgresql(keywords_tsv_file, uri, schema_name='public', table_name='keywords')</code>","text":"<p>Load the keywords_all_2024_06_26.tsv (or similar version) file into the database.</p> Source code in <code>src/bio_data_to_db/uniprot/utils.py</code> <pre><code>def keywords_tsv_to_postgresql(\n    keywords_tsv_file: str | PathLike,\n    uri: str,\n    schema_name=\"public\",\n    table_name=\"keywords\",\n):\n    \"\"\"\n    Load the keywords_all_2024_06_26.tsv (or similar version) file into the database.\n    \"\"\"\n    tsv_columns = [\n        \"Keyword ID\",\n        \"Name\",\n        \"Category\",\n        \"Gene Ontologies\",\n    ]\n    polars_columns = [\n        \"keyword_id\",\n        \"name\",\n        \"category\",\n        \"geneontology_names\",\n    ]\n    rename_dict = dict(zip(tsv_columns, polars_columns, strict=True))\n    polars_dtypes = [\n        pl.Utf8,\n        pl.Utf8,\n        pl.Utf8,\n        pl.Utf8,\n    ]\n\n    df = pl.read_csv(\n        str(keywords_tsv_file),\n        separator=\"\\t\",\n        columns=tsv_columns,\n        # NOTE: Do not use new_columns with columns. It will result in wrong data.\n        # new_columns=polars_columns,\n        schema_overrides=polars_dtypes,\n    )\n\n    df = df.rename(rename_dict)\n\n    # geneontology_names is a list of strings\n    df = df.with_columns(\n        geneontology_names=pl.col(\"geneontology_names\")\n        .str.split(\", \")\n        .cast(pl.List(pl.Utf8))\n    )\n\n    polars_write_database(\n        df, schema_name=schema_name, table_name=table_name, connection=uri\n    )\n</code></pre>"},{"location":"reference/bio_data_to_db/uniprot/utils/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> utils","text":""},{"location":"reference/bio_data_to_db/uniprot/utils/#bio_data_to_db.uniprot.utils.create_empty_table","title":"<code>create_empty_table(uri)</code>","text":"<p>Create an empty table in the database. Necessary to create the table structure before inserting data.</p> Note <p>It runs the following SQL query: <pre><code>CREATE TABLE public.uniprot_info (\n  uniprot_pk_id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,\n  accessions TEXT[],\n  names TEXT[],\n  protein_names TEXT[],\n  gene_names TEXT[],\n  organism_scientific TEXT,\n  organism_commons TEXT[],\n  organism_synonyms TEXT[],\n  ncbi_taxonomy_id INT,\n  deargen_ncbi_taxonomy_id INT,\n  lineage TEXT[],\n  keywords TEXT[],\n  geneontology_ids TEXT[],\n  geneontology_names TEXT[],\n  sequence TEXT,\n  deargen_molecular_functions TEXT[]\n)\n</code></pre></p> Source code in <code>src/bio_data_to_db/uniprot/utils.py</code> <pre><code>def create_empty_table(\n    uri: str,\n):\n    \"\"\"\n    Create an empty table in the database. Necessary to create the table structure before inserting data.\n\n    Note:\n        It runs the following SQL query:\n        ```sql\n        CREATE TABLE public.uniprot_info (\n          uniprot_pk_id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,\n          accessions TEXT[],\n          names TEXT[],\n          protein_names TEXT[],\n          gene_names TEXT[],\n          organism_scientific TEXT,\n          organism_commons TEXT[],\n          organism_synonyms TEXT[],\n          ncbi_taxonomy_id INT,\n          deargen_ncbi_taxonomy_id INT,\n          lineage TEXT[],\n          keywords TEXT[],\n          geneontology_ids TEXT[],\n          geneontology_names TEXT[],\n          sequence TEXT,\n          deargen_molecular_functions TEXT[]\n        )\n        ```\n    \"\"\"\n    uri_wo_dbname, dbname = uri.rsplit(\"/\", 1)\n    create_db_if_not_exists(uri_wo_dbname, dbname)\n    create_schema_if_not_exists(uri, \"public\")\n\n    with psycopg.connect(\n        conninfo=uri,\n    ) as conn:\n        try:\n            cursor = conn.cursor()\n            conn.autocommit = True\n            cursor.execute(\n                query=\"\"\"\n                CREATE TABLE public.uniprot_info (\n                  uniprot_pk_id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,\n                  accessions TEXT[],\n                  names TEXT[],\n                  protein_names TEXT[],\n                  gene_names TEXT[],\n                  organism_scientific TEXT,\n                  organism_commons TEXT[],\n                  organism_synonyms TEXT[],\n                  ncbi_taxonomy_id INT,\n                  deargen_ncbi_taxonomy_id INT,\n                  lineage TEXT[],\n                  keywords TEXT[],\n                  geneontology_ids TEXT[],\n                  geneontology_names TEXT[],\n                  sequence TEXT,\n                  deargen_molecular_functions TEXT[]\n                )\n            \"\"\"\n            )\n            logger.info(f\"Database '{dbname}.public.uniprot_info' created successfully\")\n\n        except psycopg.Error:\n            logger.exception(f\"Error creating database '{dbname}.public.uniprot_info'\")\n</code></pre>"},{"location":"reference/bio_data_to_db/uniprot/utils/#bio_data_to_db.uniprot.utils.create_accession_to_pk_id","title":"<code>create_accession_to_pk_id(uri)</code>","text":"<p>Create a table to map accession to uniprot_pk_id, from the uniprot_info table.</p> <p>It creates the following tables:</p> <ul> <li>accession_to_pk_id</li> <li>accession_to_pk_id_list</li> </ul> Note <p>The mapping is not unique. It is possible to have multiple uniprot_pk_id for a single accession and vice versa.</p> Source code in <code>src/bio_data_to_db/uniprot/utils.py</code> <pre><code>def create_accession_to_pk_id(uri: str):\n    \"\"\"\n    Create a table to map accession to uniprot_pk_id, from the uniprot_info table.\n\n    It creates the following tables:\n\n    - accession_to_pk_id\n    - accession_to_pk_id_list\n\n    Note:\n        The mapping is not unique. It is possible to have multiple uniprot_pk_id for a single accession and vice versa.\n    \"\"\"\n    with psycopg.connect(\n        conninfo=uri,\n    ) as conn:\n        try:\n            cursor = conn.cursor()\n            conn.autocommit = True\n            cursor.execute(\n                query=\"\"\"\n                CREATE TABLE public.accession_to_pk_id (\n                  accession TEXT,\n                  uniprot_pk_id BIGINT\n                )\n            \"\"\"\n            )\n            logger.info(\n                \"Table structure 'uniprot.public.accession_to_pk_id' created successfully\"\n            )\n\n            cursor.execute(\n                query=\"\"\"\n                INSERT INTO public.accession_to_pk_id (accession, uniprot_pk_id)\n                SELECT UNNEST(accessions), uniprot_pk_id\n                FROM public.uniprot_info\n            \"\"\"\n            )\n            logger.info(\n                \"Table 'uniprot.public.accession_to_pk_id' insert content successfully\"\n            )\n\n            cursor.execute(\n                query=\"\"\"\n                CREATE TABLE public.accession_to_pk_id_list (\n                  accession TEXT PRIMARY KEY,\n                  uniprot_pk_ids BIGINT[]\n                )\n            \"\"\"\n            )\n            logger.info(\n                \"Table structure 'uniprot.public.accession_to_pk_id_list' created successfully\"\n            )\n\n            cursor.execute(\n                query=\"\"\"\n                INSERT INTO public.accession_to_pk_id_list (accession, uniprot_pk_ids)\n                SELECT accession, ARRAY_AGG(uniprot_pk_id) AS uniprot_pk_ids\n                FROM public.accession_to_pk_id\n                GROUP BY accession;\n            \"\"\"\n            )\n            logger.info(\n                \"Table 'uniprot.public.accession_to_pk_id_list' content added successfully\"\n            )\n\n        except psycopg.Error:\n            logger.exception(\"Error creating table 'uniprot.public.accession_to_pk_id'\")\n</code></pre>"},{"location":"reference/bio_data_to_db/uniprot/utils/#bio_data_to_db.uniprot.utils.keywords_tsv_to_postgresql","title":"<code>keywords_tsv_to_postgresql(keywords_tsv_file, uri, schema_name='public', table_name='keywords')</code>","text":"<p>Load the keywords_all_2024_06_26.tsv (or similar version) file into the database.</p> Source code in <code>src/bio_data_to_db/uniprot/utils.py</code> <pre><code>def keywords_tsv_to_postgresql(\n    keywords_tsv_file: str | PathLike,\n    uri: str,\n    schema_name=\"public\",\n    table_name=\"keywords\",\n):\n    \"\"\"\n    Load the keywords_all_2024_06_26.tsv (or similar version) file into the database.\n    \"\"\"\n    tsv_columns = [\n        \"Keyword ID\",\n        \"Name\",\n        \"Category\",\n        \"Gene Ontologies\",\n    ]\n    polars_columns = [\n        \"keyword_id\",\n        \"name\",\n        \"category\",\n        \"geneontology_names\",\n    ]\n    rename_dict = dict(zip(tsv_columns, polars_columns, strict=True))\n    polars_dtypes = [\n        pl.Utf8,\n        pl.Utf8,\n        pl.Utf8,\n        pl.Utf8,\n    ]\n\n    df = pl.read_csv(\n        str(keywords_tsv_file),\n        separator=\"\\t\",\n        columns=tsv_columns,\n        # NOTE: Do not use new_columns with columns. It will result in wrong data.\n        # new_columns=polars_columns,\n        schema_overrides=polars_dtypes,\n    )\n\n    df = df.rename(rename_dict)\n\n    # geneontology_names is a list of strings\n    df = df.with_columns(\n        geneontology_names=pl.col(\"geneontology_names\")\n        .str.split(\", \")\n        .cast(pl.List(pl.Utf8))\n    )\n\n    polars_write_database(\n        df, schema_name=schema_name, table_name=table_name, connection=uri\n    )\n</code></pre>"},{"location":"reference/bio_data_to_db/utils/","title":"Index","text":""},{"location":"reference/bio_data_to_db/utils/fasta/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> fasta","text":""},{"location":"reference/bio_data_to_db/utils/fasta/#bio_data_to_db.utils.fasta.polars_standardize_fasta","title":"<code>polars_standardize_fasta(df, fasta_col='fasta', out_col='fasta')</code>","text":"<p>Remove spaces and make it uppercase of a Polars column.</p> Source code in <code>src/bio_data_to_db/utils/fasta.py</code> <pre><code>def polars_standardize_fasta(\n    df: pl.DataFrame, fasta_col: str = \"fasta\", out_col: str = \"fasta\"\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Remove spaces and make it uppercase of a Polars column.\n    \"\"\"\n    df = df.with_columns(\n        pl.col(fasta_col)\n        .str.to_uppercase()\n        .str.replace_all(\"\\n\", \"\")\n        .str.replace_all(\" \", \"\")\n        .alias(out_col)\n    )\n\n    return df\n</code></pre>"},{"location":"reference/bio_data_to_db/utils/log/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> log","text":""},{"location":"reference/bio_data_to_db/utils/log/#bio_data_to_db.utils.log.setup_logging","title":"<code>setup_logging(console_level=logging.INFO, output_files=None, file_levels=None)</code>","text":"<p>Setup logging with RichHandler and FileHandler.</p> <p>You should call this function at the beginning of your script.</p> <p>Parameters:</p> Name Type Description Default <code>console_level</code> <code>int | str</code> <p>Logging level for console. Defaults to INFO or env var PPMI_LOG_LEVEL.</p> <code>INFO</code> <code>output_files</code> <code>list[str] | None</code> <p>List of output file paths, relative to LOG_DIR. If None, use default.</p> <code>None</code> <code>file_levels</code> <code>list[int | str] | None</code> <p>List of logging levels for each output file. If None, use default.</p> <code>None</code> Source code in <code>src/bio_data_to_db/utils/log.py</code> <pre><code>def setup_logging(\n    console_level: int | str = logging.INFO,\n    output_files: list[str] | None = None,\n    file_levels: list[int | str] | None = None,\n):\n    \"\"\"\n    Setup logging with RichHandler and FileHandler.\n\n    You should call this function at the beginning of your script.\n\n    Args:\n        console_level: Logging level for console. Defaults to INFO or env var PPMI_LOG_LEVEL.\n        output_files: List of output file paths, relative to LOG_DIR. If None, use default.\n        file_levels: List of logging levels for each output file. If None, use default.\n    \"\"\"\n    if output_files is None:\n        output_files = []\n    if file_levels is None:\n        file_levels = []\n\n    assert len(output_files) == len(\n        file_levels\n    ), \"output_files and file_levels must have the same length\"\n\n    # NOTE: Initialise with NOTSET level and null device, and add stream handler separately.\n    # This way, the root logging level is NOTSET (log all), and we can customise each handler's behaviour.\n    # If we set the level during the initialisation, it will affect to ALL streams,\n    # so the file stream cannot be more verbose (lower level) than the console stream.\n    logging.basicConfig(\n        format=\"\",\n        level=logging.NOTSET,\n        stream=open(os.devnull, \"w\"),  # noqa: SIM115\n    )\n\n    # If you want to suppress logs from other modules, set their level to WARNING or higher\n    # logging.getLogger(\"slowfast.utils.checkpoint\").setLevel(logging.WARNING)\n\n    console_handler = RichHandler(\n        level=console_level,\n        show_time=True,\n        show_level=True,\n        show_path=True,\n        rich_tracebacks=True,\n        tracebacks_show_locals=True,\n        console=console,\n    )\n    console_format = logging.Formatter(\n        fmt=\"%(name)s - %(message)s\",\n        datefmt=\"%m/%d %H:%M:%S\",\n    )\n    console_handler.setFormatter(console_format)\n\n    f_format = logging.Formatter(\n        fmt=\"%(asctime)s - %(name)s: %(lineno)4d - %(levelname)s - %(message)s\",\n        datefmt=\"%y/%m/%d %H:%M:%S\",\n    )\n\n    root_logger = logging.getLogger()\n    root_logger.addHandler(console_handler)\n\n    log_paths = []\n    for output_file, file_level in zip(output_files, file_levels, strict=True):\n        Path(output_file).parent.mkdir(parents=True, exist_ok=True)\n\n        f_handler = logging.FileHandler(output_file)\n        f_handler.setLevel(file_level)\n        f_handler.setFormatter(f_format)\n\n        # Add handlers to the logger\n        root_logger.addHandler(f_handler)\n\n    for log_path in log_paths:\n        logger.info(f\"Logging to {log_path}\")\n</code></pre>"},{"location":"reference/bio_data_to_db/utils/polars/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> polars","text":""},{"location":"reference/bio_data_to_db/utils/polars/#bio_data_to_db.utils.polars.w_pbar","title":"<code>w_pbar(pbar, func)</code>","text":"<p>Apply progress bar when using <code>map_elements</code> in <code>polars</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; with tqdm(total=len(df)) as pbar:\n...     df = df.with_columns(\n...         pl.col(\"in_col\")\n...         .map_elements(w_pbar(pbar, lambda x: x + 1), return_dtype=pl.Int64)\n...     )\n</code></pre> Reference <ul> <li>https://stackoverflow.com/questions/75550124/python-polars-how-to-add-a-progress-bars-to-apply-loops</li> </ul> Source code in <code>src/bio_data_to_db/utils/polars.py</code> <pre><code>def w_pbar(pbar: tqdm.std.tqdm, func: Callable[..., Any]) -&gt; Callable[..., Any]:\n    \"\"\"\n    Apply progress bar when using `map_elements` in `polars`.\n\n    Examples:\n        &gt;&gt;&gt; with tqdm(total=len(df)) as pbar:  # doctest: +SKIP\n        ...     df = df.with_columns(\n        ...         pl.col(\"in_col\")\n        ...         .map_elements(w_pbar(pbar, lambda x: x + 1), return_dtype=pl.Int64)\n        ...     )\n\n    Reference:\n        - https://stackoverflow.com/questions/75550124/python-polars-how-to-add-a-progress-bars-to-apply-loops\n    \"\"\"\n\n    def foo(*args, **kwargs):\n        pbar.update(1)\n        return func(*args, **kwargs)\n\n    return foo\n</code></pre>"},{"location":"reference/bio_data_to_db/utils/postgresql/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> postgresql","text":""},{"location":"reference/bio_data_to_db/utils/postgresql/#bio_data_to_db.utils.postgresql.create_db_if_not_exists","title":"<code>create_db_if_not_exists(uri_wo_db, db_name, comment=None)</code>","text":"<p>Create a database if it doesn't exist.</p> Source code in <code>src/bio_data_to_db/utils/postgresql.py</code> <pre><code>def create_db_if_not_exists(uri_wo_db: str, db_name: str, comment: str | None = None):\n    \"\"\"\n    Create a database if it doesn't exist.\n    \"\"\"\n    with psycopg.connect(\n        conninfo=f\"{uri_wo_db}\",\n    ) as conn:\n        try:\n            cursor = conn.cursor()\n            conn.autocommit = True\n            cursor.execute(\n                query=sql.SQL(\"\"\"CREATE DATABASE {db_name};\"\"\").format(\n                    db_name=sql.Identifier(db_name)\n                )\n            )\n            if comment is not None:\n                cursor.execute(\n                    query=sql.SQL(\n                        \"\"\"COMMENT ON DATABASE {db_name} IS {comment};\"\"\"\n                    ).format(\n                        db_name=sql.Identifier(db_name),\n                        comment=sql.Literal(comment),\n                    )\n                )\n            logger.info(f\"Database '{db_name}' created successfully\")\n        except psycopg.errors.DuplicateDatabase:\n            logger.info(f\"Database '{db_name}' already exists, Skip creating database.\")\n            if comment is not None:\n                conn.rollback()\n                cursor = conn.cursor()\n                cursor.execute(\n                    query=sql.SQL(\n                        \"\"\"COMMENT ON DATABASE {db_name} IS {comment};\"\"\"\n                    ).format(\n                        db_name=sql.Identifier(db_name),\n                        comment=sql.Literal(comment),\n                    )\n                )\n\n        except psycopg.Error:\n            logger.exception(f\"Error creating database '{db_name}'\")\n</code></pre>"},{"location":"reference/bio_data_to_db/utils/postgresql/#bio_data_to_db.utils.postgresql.create_schema_if_not_exists","title":"<code>create_schema_if_not_exists(uri, schema_name, comment=None)</code>","text":"<p>Create a schema if it doesn't exist. The DB should already exist.</p> Source code in <code>src/bio_data_to_db/utils/postgresql.py</code> <pre><code>def create_schema_if_not_exists(uri: str, schema_name: str, comment: str | None = None):\n    \"\"\"\n    Create a schema if it doesn't exist. The DB should already exist.\n    \"\"\"\n    db_name = uri.split(\"/\")[-1]\n    with psycopg.connect(\n        conninfo=uri,\n    ) as conn:\n        try:\n            cursor = conn.cursor()\n            cursor.execute(\n                query=sql.SQL(\"\"\"CREATE SCHEMA {schema_name};\"\"\").format(\n                    schema_name=sql.Identifier(schema_name)\n                )\n            )\n            if comment is not None:\n                cursor.execute(\n                    query=sql.SQL(\n                        \"\"\"COMMENT ON SCHEMA {schema_name} IS {comment};\"\"\"\n                    ).format(\n                        schema_name=sql.Identifier(schema_name),\n                        comment=sql.Literal(comment),\n                    )\n                )\n            conn.commit()\n\n            logger.info(\n                f\"Schema '{schema_name}' created successfully in DB '{db_name}'\"\n            )\n        except psycopg.errors.DuplicateSchema:\n            logger.info(\n                f\"Schema '{schema_name}' in DB '{db_name}' already exists, Skip creating schema.\"\n            )\n            if comment is not None:\n                # cancel the transaction and try to add comment\n                conn.rollback()\n                cursor = conn.cursor()\n                cursor.execute(\n                    query=sql.SQL(\n                        \"\"\"COMMENT ON SCHEMA {schema_name} IS {comment};\"\"\"\n                    ).format(\n                        schema_name=sql.Identifier(schema_name),\n                        comment=sql.Literal(comment),\n                    )\n                )\n                conn.commit()\n        except psycopg.Error:\n            logger.exception(f\"Error creating schema '{schema_name}' in DB '{db_name}'\")\n</code></pre>"},{"location":"reference/bio_data_to_db/utils/postgresql/#bio_data_to_db.utils.postgresql.make_int_column_primary_key_identity","title":"<code>make_int_column_primary_key_identity(uri, *, schema_name='public', table_name, column_name='index')</code>","text":"<p>Make an existing index column (integer type) as primary key with auto increment (identity).</p> <p>This is used because pl.DataFrame.write_database() doesn't support writing index column as primary key. Also, it will automatically set the start value of auto increment to the max value in the column.</p> Example <p>df = pl.DataFrame({\"smiles\": [\"CCO\", \"CCN\", \"CCC\"]})  # doctest: +SKIP ... df = df.with_row_index(\"pk_id\") ... df.write_database(...) ... set_column_as_primary_key(uri=uri, table_name=\"table\", column_name=\"pk_id\") ... df2 = pl.DataFrame({\"smiles\": [\"CCC\", \"CCN\", \"CCO\"]})  # append without pk_id ... df2.write_database(..., if_table_exists=\"append\")  # it will auto increment pk_id</p> Source code in <code>src/bio_data_to_db/utils/postgresql.py</code> <pre><code>def make_int_column_primary_key_identity(\n    uri: str,\n    *,\n    schema_name: str = \"public\",\n    table_name: str,\n    column_name: str = \"index\",\n):\n    \"\"\"\n    Make an existing index column (integer type) as primary key with auto increment (identity).\n\n    This is used because pl.DataFrame.write_database() doesn't support writing index column as primary key.\n    Also, it will automatically set the start value of auto increment to the max value in the column.\n\n    Example:\n        &gt;&gt;&gt; df = pl.DataFrame({\"smiles\": [\"CCO\", \"CCN\", \"CCC\"]})  # doctest: +SKIP\n        ... df = df.with_row_index(\"pk_id\")\n        ... df.write_database(...)\n        ... set_column_as_primary_key(uri=uri, table_name=\"table\", column_name=\"pk_id\")\n        ... df2 = pl.DataFrame({\"smiles\": [\"CCC\", \"CCN\", \"CCO\"]})  # append without pk_id\n        ... df2.write_database(..., if_table_exists=\"append\")  # it will auto increment pk_id\n    \"\"\"\n    with psycopg.connect(\n        conninfo=uri,\n    ) as conn:\n        try:\n            cursor = conn.cursor()\n            # NOTE: since there are already values in the column, we need to set the start value to max+1\n\n            cursor.execute(\n                sql.SQL(\"\"\"\n            SELECT MAX({column}) FROM {table}\n            \"\"\").format(\n                    column=sql.Identifier(column_name),\n                    table=sql.Identifier(schema_name, table_name),\n                )\n            )\n            max_id = cursor.fetchone()\n            if max_id is None:\n                logger.error(\n                    f\"Error setting primary key for column '{column_name}' in table '{table_name}': no max value found\"\n                )\n                return\n\n            max_id = max_id[0]\n\n            cursor.execute(\n                sql.SQL(\"\"\"\n            ALTER TABLE {table}\n            ALTER COLUMN {column} SET NOT NULL,\n            ALTER COLUMN {column} ADD GENERATED BY DEFAULT AS IDENTITY\n              (START WITH {start_with}),\n            ADD PRIMARY KEY ({column});\n            \"\"\").format(\n                    table=sql.Identifier(schema_name, table_name),\n                    column=sql.Identifier(column_name),\n                    start_with=sql.Literal(max_id + 1),\n                )\n            )\n            conn.commit()\n\n        except psycopg.Error:\n            logger.exception(\n                f\"Error setting primary key for column '{column_name}' in table '{table_name}'\"\n            )\n</code></pre>"},{"location":"reference/bio_data_to_db/utils/postgresql/#bio_data_to_db.utils.postgresql.make_columns_primary_key","title":"<code>make_columns_primary_key(uri, *, schema_name='public', table_name, column_names)</code>","text":"<p>Make multiple columns as primary key but without auto increment (identity).</p> <p>This is similar to make_columns_unique() but with primary key constraint.</p> Source code in <code>src/bio_data_to_db/utils/postgresql.py</code> <pre><code>def make_columns_primary_key(\n    uri: str,\n    *,\n    schema_name: str = \"public\",\n    table_name: str,\n    column_names: str | Sequence[str],\n):\n    \"\"\"\n    Make multiple columns as primary key but without auto increment (identity).\n\n    This is similar to make_columns_unique() but with primary key constraint.\n    \"\"\"\n    with psycopg.connect(\n        conninfo=uri,\n    ) as conn:\n        try:\n            cursor = conn.cursor()\n\n            if isinstance(column_names, str):\n                column_names = [column_names]\n\n            cursor.execute(\n                sql.SQL(\"\"\"\n                    ALTER TABLE {table}\n                    ADD PRIMARY KEY ({columns});\n                \"\"\").format(\n                    table=sql.Identifier(schema_name, table_name),\n                    columns=sql.SQL(\",\").join(\n                        sql.Identifier(col) for col in column_names\n                    ),\n                )\n            )\n            conn.commit()\n\n        except psycopg.Error:\n            logger.exception(\n                f\"Error setting primary key for column '{column_names}' in table '{table_name}'\"\n            )\n</code></pre>"},{"location":"reference/bio_data_to_db/utils/postgresql/#bio_data_to_db.utils.postgresql.make_columns_unique","title":"<code>make_columns_unique(uri, *, schema_name='public', table_name, column_names)</code>","text":"<p>Set unique constraint on a column or columns in a table.</p> <p>If multiple columns are provided, the unique constraint will be on the combination of the columns.</p> Source code in <code>src/bio_data_to_db/utils/postgresql.py</code> <pre><code>def make_columns_unique(\n    uri: str,\n    *,\n    schema_name: str = \"public\",\n    table_name: str,\n    column_names: str | Sequence[str],\n):\n    \"\"\"\n    Set unique constraint on a column or columns in a table.\n\n    If multiple columns are provided, the unique constraint will be on the combination of the columns.\n    \"\"\"\n    with psycopg.connect(\n        conninfo=uri,\n    ) as conn:\n        try:\n            cursor = conn.cursor()\n\n            if isinstance(column_names, str):\n                column_names = [column_names]\n\n            cursor.execute(\n                query=sql.SQL(\"\"\"\n                    ALTER TABLE {table}\n                    ADD CONSTRAINT {table_unique_constraint}\n                      UNIQUE ({columns});\n                \"\"\").format(\n                    table=sql.Identifier(schema_name, table_name),\n                    table_unique_constraint=sql.Identifier(\n                        f\"{schema_name}-{table_name}-{'-'.join(column_names)}-unique_constraint\"\n                    ),\n                    columns=sql.SQL(\",\").join(\n                        sql.Identifier(col) for col in column_names\n                    ),\n                )\n            )\n            conn.commit()\n\n        except psycopg.Error:\n            logger.exception(\n                f\"Error setting unique constraint for column '{column_names}' in table '{table_name}'\"\n            )\n</code></pre>"},{"location":"reference/bio_data_to_db/utils/postgresql/#bio_data_to_db.utils.postgresql.make_large_columns_unique","title":"<code>make_large_columns_unique(uri, *, schema_name='public', table_name, column_names)</code>","text":"<p>Use this when the values are large texts, e.g. fasta sequences.</p> Reference <ul> <li>https://stackoverflow.com/questions/71379137/how-to-solve-postgresql-index-width-problem</li> </ul> Source code in <code>src/bio_data_to_db/utils/postgresql.py</code> <pre><code>def make_large_columns_unique(\n    uri: str,\n    *,\n    schema_name: str = \"public\",\n    table_name: str,\n    column_names: str | Sequence[str],\n):\n    \"\"\"\n    Use this when the values are large texts, e.g. fasta sequences.\n\n    Reference:\n        - https://stackoverflow.com/questions/71379137/how-to-solve-postgresql-index-width-problem\n    \"\"\"\n    with psycopg.connect(\n        conninfo=uri,\n    ) as conn:\n        try:\n            cursor = conn.cursor()\n\n            if isinstance(column_names, str):\n                column_names = [column_names]\n\n            cursor.execute(\n                query=sql.SQL(\"\"\"\n                CREATE UNIQUE INDEX ON {table} (\n                  {columns}\n                );\n                \"\"\").format(\n                    table=sql.Identifier(schema_name, table_name),\n                    columns=sql.SQL(\",\").join(\n                        sql.SQL(\"md5(\") + sql.Identifier(col) + sql.SQL(\")\")\n                        for col in column_names\n                    ),\n                )\n            )\n            conn.commit()\n\n        except psycopg.Error:\n            logger.exception(\n                f\"Error setting unique index for column '{column_names}' in table '{table_name}'\"\n            )\n</code></pre>"},{"location":"reference/bio_data_to_db/utils/postgresql/#bio_data_to_db.utils.postgresql.split_column_str_to_list","title":"<code>split_column_str_to_list(uri, *, schema_name='public', table_name, in_column, out_column, separator, pg_element_type='text')</code>","text":"<p>Split a string column into a list column.</p> Source code in <code>src/bio_data_to_db/utils/postgresql.py</code> <pre><code>def split_column_str_to_list(\n    uri: str,\n    *,\n    schema_name: str = \"public\",\n    table_name: str,\n    in_column: str,\n    out_column: str,\n    separator: str,\n    pg_element_type: str = \"text\",\n):\n    \"\"\"\n    Split a string column into a list column.\n    \"\"\"\n    if pg_element_type.lower() not in {\n        \"text\",\n    }:\n        raise ValueError(f\"Unsupported PostgreSQL element type: {pg_element_type}\")\n\n    list_type = sql.SQL(f\"{pg_element_type}[]\")  # type: ignore\n\n    with psycopg.connect(\n        conninfo=uri,\n    ) as conn:\n        try:\n            cursor = conn.cursor()\n\n            # split the string into a list, and write it to a new column\n            # plus remove the old column\n            cursor.execute(\n                query=sql.SQL(\"\"\"\n                ALTER TABLE {table}\n                ADD COLUMN {out_column} {list_type};\n                \"\"\").format(\n                    table=sql.Identifier(schema_name, table_name),\n                    out_column=sql.Identifier(out_column),\n                    list_type=list_type,\n                )\n            )\n            cursor.execute(\n                query=sql.SQL(\"\"\"\n                UPDATE {table}\n                SET {out_column} = STRING_TO_ARRAY({in_column}, %s)::{list_type};\n                \"\"\").format(\n                    table=sql.Identifier(schema_name, table_name),\n                    out_column=sql.Identifier(out_column),\n                    in_column=sql.Identifier(in_column),\n                    list_type=list_type,\n                ),\n                params=(separator,),\n            )\n\n            cursor.execute(\n                query=sql.SQL(\"\"\"\n                ALTER TABLE {table}\n                DROP COLUMN {in_column};\n                \"\"\").format(\n                    table=sql.Identifier(schema_name, table_name),\n                    in_column=sql.Identifier(in_column),\n                )\n            )\n            conn.commit()\n\n        except psycopg.Error:\n            logger.exception(\n                f\"Error splitting column '{in_column}' in table '{table_name}'\"\n            )\n</code></pre>"},{"location":"reference/bio_data_to_db/utils/postgresql/#bio_data_to_db.utils.postgresql.polars_write_database","title":"<code>polars_write_database(df, *, schema_name='public', table_name, connection, if_table_exists='fail')</code>","text":"<p>pl.DataFrame.write_database() but address the issue of writing unsigned and list columns to database.</p> Reference <ul> <li>https://stackoverflow.com/questions/77098480/polars-psycopg2-write-column-of-lists-to-postgresql</li> </ul> Source code in <code>src/bio_data_to_db/utils/postgresql.py</code> <pre><code>def polars_write_database(\n    df: pl.DataFrame,\n    *,\n    schema_name: str = \"public\",\n    table_name: str,\n    connection: str | Engine,\n    if_table_exists: DbWriteMode = \"fail\",\n):\n    \"\"\"\n    pl.DataFrame.write_database() but address the issue of writing unsigned and list columns to database.\n\n    Reference:\n        - https://stackoverflow.com/questions/77098480/polars-psycopg2-write-column-of-lists-to-postgresql\n    \"\"\"\n    if isinstance(connection, str):\n        connection = create_engine(connection)\n\n    columns_dtype = {col: df[col].dtype for col in df.columns}\n    column_name_to_sqlalchemy_type = {\n        col: polars_datatype_to_sqlalchemy_type(dtype)\n        for col, dtype in columns_dtype.items()\n    }\n\n    pd_df = df.to_pandas(use_pyarrow_extension_array=True)\n\n    # If any column has type list[number] in Polars, the pandas DataFrame will have a numpy array.\n    # We need to convert it to a list, because `to_sql` doesn't support numpy arrays.\n    for col, dtype in columns_dtype.items():\n        if isinstance(dtype, pl.List):\n            if isinstance(dtype.inner, pl.Utf8):\n                continue\n            pd_df[col] = pd_df[col].apply(lambda x: x.tolist())\n\n    # ic(pd_df)\n    pd_df.to_sql(\n        schema=schema_name,\n        name=table_name,\n        con=connection,\n        if_exists=if_table_exists,\n        index=False,\n        dtype=column_name_to_sqlalchemy_type,  # type: ignore\n    )\n</code></pre>"},{"location":"reference/bio_data_to_db/utils/smiles/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> smiles","text":""},{"location":"reference/bio_data_to_db/utils/smiles/#bio_data_to_db.utils.smiles.canonical_smiles_wo_salt","title":"<code>canonical_smiles_wo_salt(smiles)</code>  <code>cached</code>","text":"<p>Get the canonical SMILES without salt from the input SMILES.</p> <p>Salt is a short part separated by \".\" in the SMILES. Shared function with dti-pytorch</p> Source code in <code>src/bio_data_to_db/utils/smiles.py</code> <pre><code>@cache\ndef canonical_smiles_wo_salt(smiles: str) -&gt; str | None:\n    \"\"\"\n    Get the canonical SMILES without salt from the input SMILES.\n\n    Salt is a short part separated by \".\" in the SMILES.\n    Shared function with dti-pytorch\n    \"\"\"\n    m = Chem.MolFromSmiles(smiles)\n    if m is not None:\n        canonical_smiles = Chem.MolToSmiles(m, isomericSmiles=True, canonical=True)\n        split_smi = canonical_smiles.split(\".\")\n        if len(split_smi) &gt; 1:\n            smiles_wo_salt = max(split_smi, key=len)\n            if Chem.MolFromSmiles(smiles_wo_salt) is None:\n                smiles_wo_salt = None\n        else:\n            smiles_wo_salt = split_smi[0]\n    else:\n        smiles_wo_salt = None\n    return smiles_wo_salt\n</code></pre>"},{"location":"reference/bio_data_to_db/utils/smiles/#bio_data_to_db.utils.smiles.polars_canonical_smiles_wo_salt","title":"<code>polars_canonical_smiles_wo_salt(df, *, smiles_col='smiles', out_col='canonical_smiles_wo_salt')</code>","text":"<p>Apply canonical_smiles_wo_salt on the DataFrame with tqdm.</p> Source code in <code>src/bio_data_to_db/utils/smiles.py</code> <pre><code>def polars_canonical_smiles_wo_salt(\n    df: pl.DataFrame,\n    *,\n    smiles_col: str = \"smiles\",\n    out_col: str = \"canonical_smiles_wo_salt\",\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Apply canonical_smiles_wo_salt on the DataFrame with tqdm.\n    \"\"\"\n    with tqdm(\n        total=df.shape[0], desc=\"Converting smiles to canonical smiles without salt\"\n    ) as pbar:\n        df = df.with_columns(\n            pl.col(smiles_col)\n            .map_elements(w_pbar(pbar, canonical_smiles_wo_salt), return_dtype=pl.Utf8)\n            .alias(out_col),\n        )\n\n    return df\n</code></pre>"}]}